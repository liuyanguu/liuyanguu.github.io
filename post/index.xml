<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Welcome to my blog</title>
    <link>/post/</link>
    <description>Recent content in Posts on Welcome to my blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Shiny in Blogdown</title>
      <link>/post/2019/02/24/shiny-in-blogdown/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/24/shiny-in-blogdown/</guid>
      <description>How to embed ShinyMy Shiny app exampleHow to embed ShinySince Blogdown is for static websites, it cannot run Shiny in rmarkdown directly. According to discussion here and document here.
The only way to do it is using iframe put the code outside the chunk:
&amp;lt;iframe src=&amp;quot;https://liuyanguu.shinyapps.io/bcl_app/&amp;quot; width=1000 height=800&amp;quot;&amp;gt;&amp;lt;/iframe&amp;gt;
There is also a built-in function in knitr to do the same thing so if write in the chunk.</description>
    </item>
    
    <item>
      <title>Study shrinkage and DART in xgboost modeling using a simple dataset</title>
      <link>/post/2018/11/15/xgboost-dart-example/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/11/15/xgboost-dart-example/</guid>
      <description>DataShrinkageDART: Dropout - MARTskip_droprate_dropone_dropIt is always a good idea to study the packaged algorithm with a simple example. Inspired by my colleague Kodi’s excellent work showing how xgboost handles missing values, I tried a simple 5x2 dataset to show how shrinkage and DART influence the growth of trees in the model.
Dataset.seed(123)n0 &amp;lt;- 5X &amp;lt;- data.</description>
    </item>
    
    <item>
      <title>SHAP Visualization for XGBoost</title>
      <link>/post/2018/10/14/shap-visualization-for-xgboost/</link>
      <pubDate>Sun, 14 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/14/shap-visualization-for-xgboost/</guid>
      <description>BackgroundSummary plotSHAP plot for each feature(Under further revise)
BackgroundI will illustrate here the application of SHAP (SHapley Additive exPlnation) values to visualize the efforts of features on the outcome variable in a XGBoost model.
The function was developed by Scott Lundberg in Python Github Link and then combined into xgboost with one visualization function xgb.plot.shap. But we can make better summary figures as those functions in its Python package in more flexible ways by extracting the SHAP values and plot by ourselves.</description>
    </item>
    
    <item>
      <title>autoxgboost: Automatic XGBoost using Bayesian Optimization</title>
      <link>/post/2018/10/03/autoxgboost-bayesian-optimization/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/03/autoxgboost-bayesian-optimization/</guid>
      <description>BackgroundUsing autoxgboostNew ResultCompared to old ResultTuning over Different BoostersReturn the recommended/chosen parameters(updated on Oct 14)
BackgroundIt has been a while since my last update. I have been working on lots of interesting projects since I joined Mount Sinai in August. We have a great team here and obviously I can learn a lot from everyone around me. Most of my job so far focuses on applying machine learning techniques, mainly extreme gradient boosting and the visualization of results.</description>
    </item>
    
    <item>
      <title>Spatial data in R: Dividing raster layers into equal-area rings</title>
      <link>/post/2018/07/20/spatial-data-in-r-dividing-raster-layers-into-equal-area-rings/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/20/spatial-data-in-r-dividing-raster-layers-into-equal-area-rings/</guid>
      <description>Saturation in ten cities with equal-area ringsR Code for one cityResults for the ring saturationsAverage saturation in each ringMethodologyOriginal CodeThis data visualization example include:
* Import .img file as a raster
* Turn raster into a data.frame of points (coordinates) and values
* Dividing the points into 100 equal-area rings
* Calculate Built-up Area/Urban Extent for each ring
* Turn dataframe into raster</description>
    </item>
    
    <item>
      <title>How to Draw Heatmap with Colorful Dendrogram</title>
      <link>/post/2018/07/16/how-to-draw-heatmap-with-colorful-dendrogram/</link>
      <pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/16/how-to-draw-heatmap-with-colorful-dendrogram/</guid>
      <description>DataVersion 1: Color both the branches and labelsVersion 2: color only the labels.Version 3: If there is no color, and we do not reorder the branchesThis data visualization example include:
* Hierarchical clustering, dendrogram and heat map based on normalized odds ratios
* The dendrogram was built separately to give color to dendrogram’s branches/labels based on cluster using dendextend
* Heatmap is made by heatmap.</description>
    </item>
    
    <item>
      <title>eXtreme Gradient Boosting (XGBoost): Better than random forest or gradient boosting</title>
      <link>/post/2018/07/09/extreme-gradient-boosting-xgboost-better-than-random-forest-or-gradient-boosting/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/09/extreme-gradient-boosting-xgboost-better-than-random-forest-or-gradient-boosting/</guid>
      <description>OverviewAbout the Data1. Extreme Gradient Boosting2. Gradient boosting3. Random Forest4. Lasso5. Best SubsetCompare MSEsOriginal CodeOverviewI recently had the great pleasure to meet with Professor Allan Just and he introduced me to eXtreme Gradient Boosting (XGBoost). I have extended the earlier work on my old blog by comparing the results across XGBoost, Gradient Boosting (GBM), Random Forest, Lasso, and Best Subset.</description>
    </item>
    
    <item>
      <title>Catalog of my old blog</title>
      <link>/post/2018/07/01/catalogue-of-my-old-blog/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/01/catalogue-of-my-old-blog/</guid>
      <description>Introducing my new blog written solely in R MarkdownWhen I realized it was so convenient to write blog directly using R Markdown, I searched if there is a specific tool for it. And I found Blogdown, an R package developed by Yihui Xie, who is also the author of R Markdown.
CatalogAs a summary, I would like to create a catalog for the main topics I wrote on google blogger before July 2018.</description>
    </item>
    
  </channel>
</rss>