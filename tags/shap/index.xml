<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SHAP on Welcome to my blog</title>
    <link>/tags/shap/</link>
    <description>Recent content in SHAP on Welcome to my blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 14 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/shap/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>SHAP Visualization for XGBoost</title>
      <link>/post/2018/10/14/shap-visualization-for-xgboost/</link>
      <pubDate>Sun, 14 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/10/14/shap-visualization-for-xgboost/</guid>
      <description>BackgroundSummary plotSHAP plot for each feature(Under further revise)
BackgroundI will illustrate here the application of SHAP (SHapley Additive exPlnation) values to visualize the efforts of features on the outcome variable in a XGBoost model.
The function was developed by Scott Lundberg in Python Github Link and then combined into xgboost with one visualization function xgb.plot.shap. But we can make better summary figures as those functions in its Python package in more flexible ways by extracting the SHAP values and plot by ourselves.</description>
    </item>
    
  </channel>
</rss>